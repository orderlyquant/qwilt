{
  "hash": "24fffa801316874def09d02bd1c5b17c",
  "result": {
    "markdown": "---\ntitle: \"Better Stock Correlation Analysis\"\ndescription: |\n  Building on the previous post, with some considerations towards efficiency\n  and increasing depth of analysis.\nauthor:\n  - name: Brandon Farr\n    url: {}\ndate: 01-30-2021\ncategories:\n  - tidyquant\n  - tibbletime\n  - finance\noutput:\n  distill::distill_article:\n    self_contained: false\n    code_folding: true\ndraft: false\n---\n\n\n\n\n**Short-comings of previous analysis**\n\n- too many steps to get correlations\n- don't need to calculate all correlation pairs, just pairs including AAPL\n- point-in-time versus evolution of correlation\n- ~~needs a plot, likely interactive (`plotly`)~~\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # for tidy/dplyr work\nlibrary(rvest)      # for web-scraping\nlibrary(tidyquant)  # for quant work (tiingo pricing, tq_mutate ...)\n```\n:::\n\n\n## API Consideration - repeated pulls\n\nGetting to a completed post is an iterative process, that involves: work,\nknitting the document, assessing the current state, deciding what to add,\nthen repeating until the final version. This leads to repetitive pulls\nof the data from Tiingo. So, I begin by executing a single pull and storing\nit, so that we avoid this repetition.\n\nRun this chunk once while working on the post, then set chunk options\nto `eval=FALSE` and `echo=TRUE` so that the code displays, but is not\nexecuted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create S&P 500 constituent table\nsp500_tbl <- read_html(\n  \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n) %>%\n  html_element(\"#constituents\") %>%\n  html_table() %>%\n  janitor::clean_names()\n\nwrite_csv(sp500_tbl, \"data/sp500.csv\")\n\n# create AAPL peer pricing table\npeer_pricing_tbl <- sp500_tbl %>%\n  filter(\n    gics_sub_industry == sp500_tbl %>% filter(symbol == \"AAPL\") %>% pull(gics_sub_industry)\n  ) %>%\n  pull(symbol) %>%\n  tq_get(.x, get = \"tiingo\")\n\nwrite_csv(peer_pricing_tbl, \"data/peer_pricing.csv\")\n```\n:::\n\n\nOnce this chunk has been run successfully, just load the data from the local\ncsv`s: `sp500.csv` and `peer_pricing.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsp500_tbl <- read_csv(\"data/sp500.csv\")\npeer_pricing_tbl <- read_csv(\"data/peer_pricing.csv\")\n\nglimpse(sp500_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 505\nColumns: 9\n$ symbol                <chr> \"MMM\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", \"ATVI\", \"AD…\n$ security              <chr> \"3M Company\", \"Abbott Laboratories\", \"AbbVie Inc…\n$ sec_filings           <chr> \"reports\", \"reports\", \"reports\", \"reports\", \"rep…\n$ gics_sector           <chr> \"Industrials\", \"Health Care\", \"Health Care\", \"He…\n$ gics_sub_industry     <chr> \"Industrial Conglomerates\", \"Health Care Equipme…\n$ headquarters_location <chr> \"St. Paul, Minnesota\", \"North Chicago, Illinois\"…\n$ date_first_added      <chr> \"1976-08-09\", \"1964-03-31\", \"2012-12-31\", \"2018-…\n$ cik                   <dbl> 66740, 1800, 1551152, 815094, 1467373, 718877, 7…\n$ founded               <chr> \"1902\", \"1888\", \"2013 (1888)\", \"1981\", \"1989\", \"…\n```\n:::\n\n```{.r .cell-code}\nglimpse(peer_pricing_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,764\nColumns: 14\n$ symbol      <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"A…\n$ date        <dttm> 2020-01-31, 2020-02-03, 2020-02-04, 2020-02-05, 2020-02-0…\n$ open        <dbl> 320.93, 304.30, 315.31, 323.52, 322.57, 322.37, 314.18, 32…\n$ high        <dbl> 322.68, 313.49, 319.64, 324.76, 325.22, 323.40, 321.55, 32…\n$ low         <dbl> 308.29, 302.22, 313.63, 318.95, 320.26, 318.00, 313.85, 31…\n$ close       <dbl> 309.51, 308.66, 318.85, 321.45, 325.21, 320.03, 321.55, 31…\n$ volume      <dbl> 49897096, 43496401, 34154134, 29706718, 26356385, 29421012…\n$ adjusted    <dbl> 76.71393, 76.50326, 79.02891, 79.67334, 80.60527, 79.51223…\n$ adjHigh     <dbl> 79.97820, 77.70040, 79.22472, 80.49374, 80.60775, 80.34951…\n$ adjLow      <dbl> 76.41155, 74.90706, 77.73510, 79.05370, 79.37839, 79.00787…\n$ adjOpen     <dbl> 79.54445, 75.42260, 78.15150, 80.18640, 79.95093, 80.09361…\n$ adjVolume   <dbl> 199588384, 173985604, 136616536, 118826872, 105425540, 117…\n$ divCash     <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.77, 0.00, 0.00, 0.00, 0.00…\n$ splitFactor <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n:::\n\n```{.r .cell-code}\n# calculate returns as well\npeer_returns_tbl <- peer_pricing_tbl %>%\n  group_by(symbol) %>%\n  tq_transmute(adjusted, periodReturn, period = \"daily\", col_rename = \"return\")\n```\n:::\n\n\n## Fewer steps, fewer pairs\n\nBuild a table with AAPL as `symbol_1` and peer stocks as `symbol_2`, using\nthe fact that `left_join` will create 1-row for every match of date. This\nis a succinct way of creating all return pairs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naapl_peer_returns_tbl <-\n  peer_returns_tbl %>%\n    filter(symbol == \"AAPL\") %>%\n    select(symbol, date, return) %>%\n  left_join(\n    peer_returns_tbl %>%\n      filter(symbol != \"AAPL\") %>%\n      select(symbol, date, return),\n    by = \"date\",\n    suffix = c(\"_1\", \"_2\")\n  ) %>%\n  select(date, matches(\"symbol\"), everything()) %>%\n  arrange(symbol_2, date) %>%\n  filter(!(return_1 == 0 & return_2 == 0))  # remove days where returns == 0\n\nglimpse(aapl_peer_returns_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,506\nColumns: 5\n$ date     <dttm> 2020-02-03, 2020-02-04, 2020-02-05, 2020-02-06, 2020-02-07, …\n$ symbol_1 <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL…\n$ symbol_2 <chr> \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\"…\n$ return_1 <dbl> -0.0027462764, 0.0330136720, 0.0081543045, 0.0116969980, -0.0…\n$ return_2 <dbl> 0.015075377, 0.019801980, 0.032593620, 0.002014775, -0.012064…\n```\n:::\n:::\n\n\nNow, calculating correlations is simple via a single `summarize` call. *Note:\nthe `use` parameter of `cor` is left as the default \"everything\" which will\ncause `NA` to appear if there are any data issues. This is preferable to just\nreturning a result, as we don't have any data robustness built into the data\npipeline.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\naapl_peer_returns_tbl %>%\n  group_by(symbol_2) %>%\n  summarize(\n    cor = cor(return_1, return_2)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  symbol_2   cor\n  <chr>    <dbl>\n1 HPE      0.477\n2 HPQ      0.498\n3 NTAP     0.537\n4 STX      0.548\n5 WDC      0.472\n6 XRX      0.460\n```\n:::\n:::\n\n\nIt can be seen that we are getting the same results with less effort, e.g.\nno `pivot`ing when comparing the above to the results shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeer_returns_tbl %>% \n  \n  # put returns into columns\n  pivot_wider(names_from = symbol, values_from = return) %>%\n  \n  # first day return is 0, so remove row\n  slice(-1) %>%\n  \n  # remove date column, so you can call `cor` on entire tibble\n  select(-date) %>%\n  \n  # calculate correlations\n  cor() %>%\n  \n  # `cor` returns a matrix, convert back into tibble\n  as_tibble(rownames = \"symbol_1\") %>%\n  \n  # transform into tidy format for easier processing\n  pivot_longer(-1, names_to = \"symbol_2\", values_to = \"cor\") %>%\n  \n  # remove the correlations of a stock with itself\n  filter(!(symbol_1 == symbol_2)) %>%\n  \n  # group by symbol_1 and arrange descending by correlation\n  group_by(symbol_1) %>%\n  arrange(desc(cor)) %>%\n  \n  # look at first row in each group == highest correlated stock\n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 3\n# Groups:   symbol_1 [7]\n  symbol_1 symbol_2   cor\n  <chr>    <chr>    <dbl>\n1 AAPL     STX      0.548\n2 HPE      NTAP     0.712\n3 HPQ      XRX      0.781\n4 NTAP     HPE      0.712\n5 STX      WDC      0.705\n6 WDC      STX      0.705\n7 XRX      HPQ      0.781\n```\n:::\n:::\n\n\n## Correlation evolution\n\nThe above analysis produces a single estimate of correlation using all of the\ndata in the dataset. Since the purpose of calculating these correlations is\nto find a substitute security for a short time frame, 30 days, it is probably\nbetter to have a sense of how stable the relationship between the two stocks\nis through time.\n\nFor this, we will use the `rollify` function from the `tibbletime` package.\nUsing the examples given in the\n[documentation](https://business-science.github.io/tibbletime/reference/rollify.html) \nmakes this relatively simple\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibbletime)\n\ncor_roll <- rollify(~cor(.x, .y), window = 30)\n\naapl_cor_tbl <-\n  aapl_peer_returns_tbl %>%\n  group_by(symbol_2) %>%\n  mutate(cor = cor_roll(return_1, return_2))\n```\n:::\n\n\n`tibbletime` really is a great convenience.\n\nNotice how the correlations are noisy (expected), but have in general declined\nsignificantly (unexpected).\n\n\n::: {.cell}\n\n```{.r .cell-code}\naapl_cor_tbl %>%\n  ggplot(aes(x = date, y = cor, color = symbol_2)) +\n  geom_hline(yintercept = 0, color = \"gray40\") +\n  geom_line() +\n  ylim(-1, 1) +\n  labs(\n    title = \"AAPL Stock Correlation\",\n    subtitle = \"with GICS sub-industry peers\",\n    caption = \"30-trading day rolling windows\",\n    x = NULL, y = NULL, color = NULL\n  ) +\n  theme_tq() +\n  scale_color_tq() +\n  # for legend to have 1 row\n  guides(col = guide_legend(nrow = 1))\n```\n\n::: {.cell-output-display}\n![](better-stock-correlation-analysis_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nA smoothed version makes this trend a little clearer.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](better-stock-correlation-analysis_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "better-stock-correlation-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}